{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install captum"
      ],
      "metadata": {
        "id": "06llpqtWkUnK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "eb64cd64-77d8-4ccf-a0e6-db510e7150ea"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: captum in /usr/local/lib/python3.10/dist-packages (0.6.0)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from captum) (3.7.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from captum) (1.22.4)\n",
            "Requirement already satisfied: torch>=1.6 in /usr/local/lib/python3.10/dist-packages (from captum) (2.0.1+cu118)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.6->captum) (3.12.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch>=1.6->captum) (4.5.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.6->captum) (1.11.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.6->captum) (3.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.6->captum) (3.1.2)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.6->captum) (2.0.0)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.6->captum) (3.25.2)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.6->captum) (16.0.5)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->captum) (1.0.7)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib->captum) (0.11.0)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->captum) (4.39.3)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->captum) (1.4.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->captum) (23.1)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->captum) (8.4.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->captum) (3.0.9)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib->captum) (2.8.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib->captum) (1.16.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.6->captum) (2.1.2)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.6->captum) (1.3.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-trwjDDgqOXK",
        "outputId": "309e3bba-e0c0-4caf-cc4a-2f1ad2d47156"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import json\n",
        "import torch.nn.functional as F\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "import torchvision.models as models\n",
        "\n",
        "import urllib.request\n",
        "from PIL import Image\n",
        "from google.colab import files\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torchvision import transforms\n",
        "\n",
        "\n",
        "from captum.attr import IntegratedGradients,DeepLift\n",
        "from captum.attr import visualization as viz"
      ],
      "metadata": {
        "id": "4-RPugaAjOjc"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Downloads the list of classes/labels for ImageNet datase\n",
        "import urllib.request\n",
        "\n",
        "url = \"https://s3.amazonaws.com/deep-learning-models/image-models/imagenet_class_index.json\"\n",
        "save_path = \"drive/MyDrive/imagenet_class_index.json\"  # Replace with the desired save path\n",
        "\n",
        "urllib.request.urlretrieve(url, save_path)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E-u_FIeyeNgi",
        "outputId": "0067f91c-b3a2-4022-a819-d76079d8c21b"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('drive/MyDrive/imagenet_class_index.json',\n",
              " <http.client.HTTPMessage at 0x7f6406f7d390>)"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import drive.MyDrive.pytorch_viz as viz\n",
        "\n",
        "with open(\"drive/MyDrive/imagenet_class_index.json\") as json_data:\n",
        "    idx_to_labels = json.load(json_data)\n"
      ],
      "metadata": {
        "id": "Hw7gDOKek-Lb"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "preprocess = transforms.Compose([\n",
        "    transforms.Resize((299, 299)),  # Resize the image to match the model's input size\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
        "])\n",
        "\n",
        "# Download the image from the internet\n",
        "image_url = \"https://images.unsplash.com/photo-1495360010541-f48722b34f7d?ixlib=rb-4.0.3&ixid=M3wxMjA3fDB8MHxzZWFyY2h8NHx8Y2F0fGVufDB8fDB8fHww&auto=format&fit=crop&w=500&q=60\"\n",
        "image_path = \"cat.jpg\"\n",
        "urllib.request.urlretrieve(image_url, image_path)\n",
        "\n",
        "# Load and preprocess the input image\n",
        "image = Image.open(image_path)\n",
        "image_tensor = preprocess(image).unsqueeze(0)"
      ],
      "metadata": {
        "id": "nKh_dKtk5E5A"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Download models\n",
        "model_resnet = torch.hub.load('pytorch/vision:v0.10.0', 'resnet50', pretrained=True)\n",
        "model_inception = torch.hub.load('pytorch/vision:v0.10.0', 'inception_v3', pretrained=True)\n",
        "model_inception.eval()\n",
        "model_vgg = torch.hub.load('pytorch/vision:v0.10.0', 'vgg16', pretrained=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sfErmBJJfDaj",
        "outputId": "f688cb7e-554d-4a83-c353-51b5e68f56d5"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Using cache found in /root/.cache/torch/hub/pytorch_vision_v0.10.0\n",
            "Using cache found in /root/.cache/torch/hub/pytorch_vision_v0.10.0\n",
            "Using cache found in /root/.cache/torch/hub/pytorch_vision_v0.10.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_resnet.eval()\n",
        "with torch.no_grad():\n",
        "    logits_resnet = model_resnet(image_tensor)"
      ],
      "metadata": {
        "id": "lQJ0b-l59L7p"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prediction_score, target_resnet = torch.topk(torch.softmax(logits_resnet, dim=1), k=5)\n"
      ],
      "metadata": {
        "id": "PjY6MtfW9PDY"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "target_resnet[0][0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z4UJ8PT5CD0l",
        "outputId": "3a965eb9-581e-48ff-8572-c921f064d571"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(281)"
            ]
          },
          "metadata": {},
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Print the top 5 labels and their probabilities\n",
        "for prob, idx in zip(prediction_score[0], target_resnet[0]):\n",
        "  print(idx_to_labels[str(idx.item())][1]+\" \"+ str(prob.squeeze().item()))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jiFOkJMIrwlB",
        "outputId": "fe0de99e-7312-405e-8b30-e5f45a8d75fd"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tabby 0.3873326778411865\n",
            "tub 0.1333470344543457\n",
            "bathtub 0.11723610758781433\n",
            "Egyptian_cat 0.10701601952314377\n",
            "tiger_cat 0.10457748174667358\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_vgg.eval()\n",
        "with torch.no_grad():\n",
        "    logits_vgg = model_vgg(image_tensor)"
      ],
      "metadata": {
        "id": "G05HMmIQ68pn"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prediction_score, target_vgg = torch.topk(torch.softmax(logits_vgg, dim=1), k=5)\n"
      ],
      "metadata": {
        "id": "chnxLiUr9ghW"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Print the top 5 labels and their probabilities\n",
        "for prob, idx in zip(prediction_score[0], target_vgg[0]):\n",
        "  print(idx_to_labels[str(idx.item())][1]+\" \"+ str(prob.squeeze().item()))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bDOB9M-29lzO",
        "outputId": "1abf4629-8ac1-4628-e3bb-97dcc3a02e75"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tabby 0.2592127323150635\n",
            "tub 0.21564650535583496\n",
            "bathtub 0.16611334681510925\n",
            "Egyptian_cat 0.10538455843925476\n",
            "tiger_cat 0.09683449566364288\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_inception.eval()\n",
        "with torch.no_grad():\n",
        "    logits_inception = model_inception(image_tensor)"
      ],
      "metadata": {
        "id": "objoiUhM9uOU"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prediction_score, target_inception = torch.topk(torch.softmax(logits_vgg, dim=1), k=5)\n"
      ],
      "metadata": {
        "id": "D7FwzdyE9l7z"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.topk(prediction_score, 1).indices.item()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tzjEdkBCBK98",
        "outputId": "50bd5ca5-76a7-4db0-c464-de6e90e68a75"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Print the top 5 labels and their probabilities\n",
        "for prob, idx in zip(prediction_score[0], target_inception[0]):\n",
        "  print(idx_to_labels[str(idx.item())][1]+\" \"+ str(prob.squeeze().item()))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OKbh2n529gi7",
        "outputId": "87532601-47ec-43d6-f816-acbd3f0edb34"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tabby 0.2592127323150635\n",
            "tub 0.21564650535583496\n",
            "bathtub 0.16611334681510925\n",
            "Egyptian_cat 0.10538455843925476\n",
            "tiger_cat 0.09683449566364288\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Lets start interpreting the models\n",
        "\n",
        "\n",
        "1.   Using Integrated Gradient\n",
        "2.   Using DEEPLIFT\n",
        "\n"
      ],
      "metadata": {
        "id": "DrEHdatI97qt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "create a baseline against which the model is going to get tallied.\n",
        "here, we will create a baseline of all zeros\n",
        "'''\n",
        "baseline = torch.zeros(image_tensor.shape)"
      ],
      "metadata": {
        "id": "8nDMCcHT9glO"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "baseline.shape ## [1, 3, 224, 224] means one image with 3 channels and 224x224 resolution"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MY2AYltN-PhZ",
        "outputId": "51666e9f-9c12-4279-ee40-afec20646f32"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([1, 3, 299, 299])"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Integrated Gradient\n"
      ],
      "metadata": {
        "id": "iUmaTk18-VSR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "IG_resnet = IntegratedGradients(model_resnet)\n",
        "attrs_ig_resnet = IG_resnet.attribute(image_tensor, target=target_resnet[0][0],n_steps=10)\n",
        "\n",
        "IG_vgg = IntegratedGradients(model_vgg)\n",
        "attrs_ig_vgg = IG_vgg.attribute(image_tensor, target=target_vgg[0][0],n_steps=10)\n",
        "\n",
        "IG_inception = IntegratedGradients(model_inception)\n",
        "attrs_ig_inception = IG_inception.attribute(image_tensor, target=target_inception[0][0],n_steps=10)"
      ],
      "metadata": {
        "id": "dzSmgrQc9gnT"
      },
      "execution_count": 74,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## define a custom heatmap (not necessary, can use predefined heatmap too)\n",
        "## have a look at the original image and model predicitions\n",
        "from matplotlib.colors import LinearSegmentedColormap\n",
        "default_cmap = LinearSegmentedColormap.from_list('custom blue',\n",
        "                                                 [(0, '#ffffff'),\n",
        "                                                  (0.25, '#000000'),\n",
        "                                                  (1, '#000000')], N=256)"
      ],
      "metadata": {
        "id": "7Ks8N7reCdHu"
      },
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "viz.visualize_image_attr_multiple(np.transpose(attrs_lrp_vgg.squeeze().cpu().detach().numpy(), (1,2,0)),\n",
        "                             np.transpose(image_tensor.squeeze().cpu().detach().numpy(), (1,2,0)),\n",
        "                             [\"original_image\", \"heat_map\"],\n",
        "                             [\"all\", \"positive\"],\n",
        "                             cmap=default_cmap,\n",
        "                             show_colorbar=True,\n",
        "                             outlier_perc=1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ItaRBv0nGejV",
        "outputId": "3938d438-77d5-48b1-d923-edd19f118257"
      },
      "execution_count": 80,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:matplotlib.image:Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(<Figure size 500x500 with 4 Axes>, array([<Axes: >, <Axes: >], dtype=object))"
            ]
          },
          "metadata": {},
          "execution_count": 80
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "viz.visualize_img(attrs_ig_resnet, image_tensor, default_cmap,  titles=['Integrated Gradient on ResNet', ''])\n",
        "viz.visualize_img(attrs_ig_vgg, image_tensor, default_cmap,  titles=['Integrated Gradient on VGG', ''])\n",
        "viz.visualize_img(attrs_ig_inception, image_tensor, default_cmap,  titles=['Integrated Gradient on Inception', ''])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "plaSu6pGCdJb",
        "outputId": "40e614f5-8763-40e3-cf03-138f5f084faf"
      },
      "execution_count": 92,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:matplotlib.image:Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
            "WARNING:matplotlib.image:Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
            "WARNING:matplotlib.image:Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(<Figure size 500x500 with 2 Axes>,\n",
              " array([<Axes: title={'center': 'Integrated Gradient on Inception'}>,\n",
              "        <Axes: >], dtype=object))"
            ]
          },
          "metadata": {},
          "execution_count": 92
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# DL_resnet = DeepLift(model_resnet)\n",
        "# attrs_dl_resnet = DL_resnet.attribute(image_tensor, target=target_resnet[0][0], )\n",
        "\n",
        "DL_vgg = DeepLift(model_vgg)\n",
        "attrs_dl_vgg = DL_vgg.attribute(image_tensor, target=target_vgg[0][0], )\n",
        "\n",
        "DL_inception = DeepLift(model_inception)\n",
        "attrs_dl_inception = DL_inception.attribute(image_tensor, target=target_inception[0][0], )"
      ],
      "metadata": {
        "id": "4iDc38o-_JtX"
      },
      "execution_count": 95,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# viz.visualize_img(attrs_dl_resnet, image_tensor, default_cmap,  titles=['Integrated Gradient on ResNet', ''])\n",
        "viz.visualize_img(attrs_dl_vgg, image_tensor, default_cmap,  titles=['Integrated Gradient on VGG', ''])\n",
        "viz.visualize_img(attrs_dl_inception, image_tensor, default_cmap,  titles=['Integrated Gradient on Inception', ''])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0kAnDU5dJq9u",
        "outputId": "56eca3c7-fde8-45e3-affb-f055d8ca0b8d"
      },
      "execution_count": 96,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:matplotlib.image:Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
            "WARNING:matplotlib.image:Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(<Figure size 500x500 with 2 Axes>,\n",
              " array([<Axes: title={'center': 'Integrated Gradient on Inception'}>,\n",
              "        <Axes: >], dtype=object))"
            ]
          },
          "metadata": {},
          "execution_count": 96
        }
      ]
    }
  ]
}